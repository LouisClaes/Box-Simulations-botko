"""
Hybrid Heuristic + DRL for 3D Online Bin Packing
=================================================

This file combines the vision-based DRL approach from Deep-Pack (Kundu et al.
2019) with heuristic action space filtering, creating a hybrid system that is
both scalable and learnable.

Key insight: Deep-Pack uses raw pixel-level actions (W*H+1), which does not
scale. Verma et al. (2020) showed that pre-filtering actions via heuristics
before applying DRL dramatically improves both scalability and performance.

This hybrid approach:
  1. Uses EMS (Empty Maximal Spaces) or Extreme Points to generate candidate
     placements (heuristic component)
  2. Uses a DRL agent to score and select among candidates (learning component)
  3. Uses the Deep-Pack heightmap representation for state (vision component)

This combines the best of:
  - Deep-Pack: vision-based state, learning-based placement
  - Ha et al. (2017): EMS-based space management, DBLF prioritization
  - Verma et al. (2020): action space reduction before DRL
  - Zhao et al. (2020): stability-aware constraint handling

Reference sections from overview KB:
  - Section 9: Empty Space Management (EMS, extreme points)
  - Section 10: Placement rules (DBLF, corner distances, DFTRC)
  - Section 11: ML approaches (Deep-Pack, Verma, Zhao)
  - Section 14: Hyper-heuristics (selecting between placement rules)

Related files:
  - python/deep_rl/deep_pack_3d_coding_ideas.py   (core DRL)
  - python/semi_online_buffer/buffer_aware_packing.py (buffer management)
  - python/heuristics/  (baseline heuristic implementations)
"""

# =============================================================================
# SECTION 1: EXTREME POINTS ON HEIGHTMAP
# =============================================================================

"""
1.1 Extreme Points for Heightmap-Based 3D Packing

In traditional 3D packing (voxel-based), extreme points are generated by
projecting the right, back, and top faces of placed items (Crainic et al.
2008, see overview KB Section 9.3).

For heightmap-based packing, the situation is simpler:
  - Items always rest on the current surface (height = max of footprint region)
  - Extreme points are 2D positions (x, y) on the bin floor
  - The z-coordinate is determined by the heightmap: z = max(hm[x:x+w, y:y+d])

Extreme point generation on heightmap:
  1. Start with corner (0, 0)
  2. After placing item at (px, py) with size (pw, pd):
     - Add point (px + pw, py)     -- right edge
     - Add point (px, py + pd)     -- back edge
     - Add point (px + pw, py + pd) -- diagonal corner
  3. Also: scan heightmap for height transitions (step edges)
     - Where heightmap changes value, the transition point is a candidate
  4. Remove points that are occupied (heightmap > 0 at that exact cell
     does NOT disqualify -- the item would simply rest higher)
  5. Remove duplicate points
"""

# import numpy as np
# from typing import List, Tuple, Set
#
# def generate_extreme_points_heightmap(bin_state: 'BinState') -> List[Tuple[int, int]]:
#     \"\"\"
#     Generate extreme points from a heightmap-based bin state.
#
#     These are candidate (x, y) positions where the top-left corner of a
#     new item could be placed.
#
#     Returns: List of (x, y) tuples, deduplicated.
#     \"\"\"
#     W, D = bin_state.bin_width, bin_state.bin_depth
#     hm = bin_state.heightmap
#     points: Set[Tuple[int, int]] = set()
#
#     # Origin is always an extreme point
#     points.add((0, 0))
#
#     # From each placed item: generate edge and corner points
#     for p in bin_state.placements:
#         bw = int(p.placed_width)
#         bd = int(p.placed_depth)
#         px, py = p.x, p.y
#
#         if px + bw < W:
#             points.add((px + bw, py))       # right edge
#         if py + bd < D:
#             points.add((px, py + bd))       # back edge
#         if px + bw < W and py + bd < D:
#             points.add((px + bw, py + bd))  # diagonal corner
#
#     # Additionally: scan for height transitions
#     # A height transition at (x, y) means the heightmap changes between
#     # (x, y) and a neighbor -- this creates a "step" where an item could sit
#     for x in range(W - 1):
#         for y in range(D - 1):
#             # Horizontal transition
#             if hm[x, y] != hm[x + 1, y]:
#                 points.add((x + 1, y))
#             # Vertical transition
#             if hm[x, y] != hm[x, y + 1]:
#                 points.add((x, y + 1))
#
#     # Filter: only keep points within bin bounds
#     valid_points = [(x, y) for (x, y) in points if 0 <= x < W and 0 <= y < D]
#
#     return valid_points


# =============================================================================
# SECTION 2: CANDIDATE GENERATION AND SCORING
# =============================================================================

"""
2.1 Candidate Placement Generation

For each extreme point and each orientation of the current item,
check feasibility and compute a heuristic score.

Feasibility criteria:
  1. Item fits within bin boundaries: x + w <= W, y + d <= D
  2. Item height fits: max(hm[x:x+w, y:y+d]) + h <= H
  3. Stability: support_ratio >= min_support (e.g., 0.7)

Heuristic scoring (multiple options, selectable):

  DBLF score: prioritize min x, then min z, then min y
  Corner distance score: min Manhattan distance to bin corner
  DFTRC score: max distance to front-top-right corner
  Deep-Pack score: predicted cluster_size * compactness

The DRL agent will learn to choose among these pre-scored candidates,
potentially learning when DBLF is better vs. corner distance, etc.
This is essentially a LEARNED HYPER-HEURISTIC (see overview KB Section 14).
"""

# @dataclass
# class CandidatePlacement:
#     \"\"\"A feasible placement candidate with heuristic scores.\"\"\"
#     x: int
#     y: int
#     z: float                          # rest height (computed from heightmap)
#     orientation: Tuple[float, float, float]  # (w, d, h) after rotation
#     support_ratio: float              # stability measure
#     dblf_score: float                 # DBLF heuristic score
#     corner_distance_score: float      # corner distance heuristic score
#     dftrc_score: float                # DFTRC heuristic score
#     contact_ratio: float              # fraction of surfaces touching neighbors
#     heightmap_variance_delta: float   # change in hm variance after placement
#
#     def feature_vector(self) -> np.ndarray:
#         \"\"\"
#         Feature vector for this candidate, to be fed to the DRL agent
#         alongside the visual state.
#         \"\"\"
#         return np.array([
#             self.x, self.y, self.z,
#             self.orientation[0], self.orientation[1], self.orientation[2],
#             self.support_ratio,
#             self.dblf_score,
#             self.corner_distance_score,
#             self.dftrc_score,
#             self.contact_ratio,
#             self.heightmap_variance_delta,
#         ], dtype=np.float32)


# def generate_candidates(bin_state: 'BinState', box: 'Box',
#                          min_support: float = 0.5,
#                          max_candidates: int = 100) -> List[CandidatePlacement]:
#     \"\"\"
#     Generate and score all feasible candidate placements.
#
#     Steps:
#       1. Get extreme points
#       2. For each (extreme_point, orientation): check feasibility
#       3. Score feasible candidates with multiple heuristics
#       4. Sort by primary heuristic and return top-k
#     \"\"\"
#     extreme_pts = generate_extreme_points_heightmap(bin_state)
#     orientations = box.get_orientations("vertical_only")
#     hm = bin_state.heightmap
#     W, D = bin_state.bin_width, bin_state.bin_depth
#     H = bin_state.bin_height
#
#     candidates = []
#
#     for (ex, ey) in extreme_pts:
#         for orient in orientations:
#             bw, bd, bh = orient
#             bw_i, bd_i = int(bw), int(bd)
#
#             # Feasibility check 1: within bounds
#             if ex + bw_i > W or ey + bd_i > D:
#                 continue
#
#             # Rest height
#             footprint = hm[ex:ex+bw_i, ey:ey+bd_i]
#             z_rest = float(np.max(footprint))
#
#             # Feasibility check 2: height limit
#             if z_rest + bh > H:
#                 continue
#
#             # Stability check: support ratio
#             if footprint.size > 0:
#                 support_ratio = float(np.sum(footprint == z_rest)) / footprint.size
#             else:
#                 support_ratio = 0.0
#
#             # Feasibility check 3: minimum support
#             if support_ratio < min_support:
#                 continue
#
#             # --- Heuristic scores ---
#
#             # DBLF: prefer min x, then min z, then min y
#             # Normalize to [0, 1] range, higher = better
#             dblf = (1 - ex/W) * 0.5 + (1 - z_rest/H) * 0.3 + (1 - ey/D) * 0.2
#
#             # Corner distance: min Manhattan distance to nearest bin corner
#             corners = [(0, 0), (W, 0), (0, D), (W, D)]
#             min_dist = min(abs(ex - cx) + abs(ey - cy) for cx, cy in corners)
#             max_possible = W + D
#             corner_dist_score = 1 - min_dist / max_possible  # higher = closer to corner
#
#             # DFTRC: distance to front-top-right corner (W, D, H)
#             # Maximize distance from FTR = place in back-bottom-left
#             dftrc = (abs(ex - W) + abs(ey - D) + abs(z_rest - H)) / (W + D + H)
#
#             # Contact ratio: fraction of item surface touching neighbors/walls
#             contact = compute_contact_ratio(bin_state, ex, ey, z_rest, bw_i, bd_i, bh)
#
#             # Heightmap variance change
#             hm_copy = hm.copy()
#             hm_copy[ex:ex+bw_i, ey:ey+bd_i] = z_rest + bh
#             var_before = float(np.var(hm))
#             var_after = float(np.var(hm_copy))
#             var_delta = var_before - var_after  # positive = improvement
#
#             candidates.append(CandidatePlacement(
#                 x=ex, y=ey, z=z_rest,
#                 orientation=orient,
#                 support_ratio=support_ratio,
#                 dblf_score=dblf,
#                 corner_distance_score=corner_dist_score,
#                 dftrc_score=dftrc,
#                 contact_ratio=contact,
#                 heightmap_variance_delta=var_delta,
#             ))
#
#     # Sort by DBLF as primary (others used as features for DRL)
#     candidates.sort(key=lambda c: c.dblf_score, reverse=True)
#     return candidates[:max_candidates]


# def compute_contact_ratio(bin_state, x, y, z, bw, bd, bh) -> float:
#     \"\"\"
#     Compute what fraction of the item's surface area is in contact with
#     other items or bin walls.
#
#     This is the 3D extension of Deep-Pack's adjacency concept.
#     \"\"\"
#     W, D = bin_state.bin_width, bin_state.bin_depth
#     H = bin_state.bin_height
#     hm = bin_state.heightmap
#
#     total_surface = 2 * (bw * bd + bw * bh + bd * bh)
#     contact = 0.0
#
#     # Bottom face: in contact if resting on floor or items
#     if z == 0:
#         contact += bw * bd  # floor contact
#     else:
#         footprint = hm[x:x+bw, y:y+bd]
#         contact += float(np.sum(footprint == z)) / (bw * bd) * (bw * bd)
#
#     # Wall contacts
#     if x == 0: contact += bd * bh
#     if x + bw == W: contact += bd * bh
#     if y == 0: contact += bw * bh
#     if y + bd == D: contact += bw * bh
#
#     # Side contacts with other items (simplified)
#     # Check neighboring columns
#     if x > 0:
#         left_col = hm[x-1, y:y+bd]
#         overlap_height = np.minimum(left_col, z + bh) - np.maximum(0, z)
#         contact += float(np.sum(np.maximum(overlap_height, 0)))
#     if x + bw < W:
#         right_col = hm[x+bw, y:y+bd]
#         overlap_height = np.minimum(right_col, z + bh) - np.maximum(0, z)
#         contact += float(np.sum(np.maximum(overlap_height, 0)))
#     if y > 0:
#         front_col = hm[x:x+bw, y-1]
#         overlap_height = np.minimum(front_col, z + bh) - np.maximum(0, z)
#         contact += float(np.sum(np.maximum(overlap_height, 0)))
#     if y + bd < D:
#         back_col = hm[x:x+bw, y+bd]
#         overlap_height = np.minimum(back_col, z + bh) - np.maximum(0, z)
#         contact += float(np.sum(np.maximum(overlap_height, 0)))
#
#     return min(contact / total_surface, 1.0) if total_surface > 0 else 0.0


# =============================================================================
# SECTION 3: HYBRID DRL AGENT (CANDIDATE SCORING)
# =============================================================================

"""
3.1 Architecture: Candidate Scoring Network

Instead of outputting Q-values for every pixel (Deep-Pack) or every
candidate position (standard DQN), we use a candidate scoring approach:

  1. CNN processes the heightmap state -> visual features (shared backbone)
  2. Each candidate's feature vector is concatenated with visual features
  3. A small scoring MLP outputs a Q-value for each candidate
  4. Select the candidate with highest Q-value

This is similar to the "attention over candidates" approach and scales
naturally to any number of candidates.

Architecture:
  [Heightmap State] -> CNN backbone -> visual_features (256-dim)
  [Candidate features] -> MLP encoder -> cand_features (64-dim)
  [visual_features || cand_features] -> Scoring MLP -> Q-value (scalar)

The scoring is done for each candidate independently (but can be batched).
"""

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
#
# class HybridCandidateScoringNet(nn.Module):
#     \"\"\"
#     Hybrid network that scores heuristic-generated candidate placements.
#
#     Combines:
#       - Visual features from heightmap (CNN backbone, a la Deep-Pack)
#       - Heuristic features for each candidate (DBLF, support, contact, etc.)
#
#     Output: Q-value for each candidate placement
#     \"\"\"
#     def __init__(self, in_channels=5, grid_w=40, grid_d=40,
#                  candidate_feature_dim=12):
#         super().__init__()
#
#         # Visual backbone (processes heightmap channels)
#         self.backbone = nn.Sequential(
#             nn.Conv2d(in_channels, 32, 3, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#             nn.Conv2d(32, 64, 3, padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(2),
#             nn.Conv2d(64, 128, 3, padding=1),
#             nn.ReLU(),
#             nn.AdaptiveAvgPool2d((4, 4)),  # fixed output regardless of input size
#         )
#         visual_feature_dim = 128 * 4 * 4  # 2048
#
#         # Visual feature projection
#         self.visual_proj = nn.Sequential(
#             nn.Linear(visual_feature_dim, 256),
#             nn.ReLU(),
#         )
#
#         # Candidate feature encoder
#         self.cand_encoder = nn.Sequential(
#             nn.Linear(candidate_feature_dim, 64),
#             nn.ReLU(),
#             nn.Linear(64, 64),
#             nn.ReLU(),
#         )
#
#         # Joint scoring head
#         self.scorer = nn.Sequential(
#             nn.Linear(256 + 64, 128),
#             nn.ReLU(),
#             nn.Linear(128, 64),
#             nn.ReLU(),
#             nn.Linear(64, 1),  # single Q-value per candidate
#         )
#
#     def forward(self, state: torch.Tensor,
#                 candidate_features: torch.Tensor,
#                 candidate_mask: torch.Tensor) -> torch.Tensor:
#         \"\"\"
#         Args:
#             state: (batch, channels, W, D) heightmap state
#             candidate_features: (batch, max_candidates, feature_dim) per-candidate features
#             candidate_mask: (batch, max_candidates) binary mask (1=valid candidate)
#
#         Returns:
#             q_values: (batch, max_candidates) Q-value for each candidate
#         \"\"\"
#         batch_size, max_cands, feat_dim = candidate_features.shape
#
#         # Extract visual features
#         vis = self.backbone(state)              # (batch, 128, 4, 4)
#         vis = vis.flatten(1)                    # (batch, 2048)
#         vis = self.visual_proj(vis)             # (batch, 256)
#
#         # Encode candidates
#         cand = self.cand_encoder(candidate_features)  # (batch, max_cands, 64)
#
#         # Expand visual features for each candidate
#         vis_expanded = vis.unsqueeze(1).expand(-1, max_cands, -1)  # (batch, max_cands, 256)
#
#         # Concatenate and score
#         combined = torch.cat([vis_expanded, cand], dim=-1)  # (batch, max_cands, 320)
#         q_values = self.scorer(combined).squeeze(-1)         # (batch, max_cands)
#
#         # Mask invalid candidates
#         q_values = q_values.masked_fill(candidate_mask == 0, -1e9)
#
#         return q_values


# =============================================================================
# SECTION 4: TRAINING WITH HYBRID APPROACH
# =============================================================================

"""
4.1 Training Loop Differences from Pure Deep-Pack

Deep-Pack:
  - State: binary image
  - Action: pixel index
  - Fixed action space per episode

Hybrid approach:
  - State: multi-channel heightmap
  - Action: candidate index (from variable-size candidate set)
  - Variable action space per step (different number of feasible candidates)
  - Additional candidate features provided alongside visual state

Key training considerations:
  1. Variable action space: pad to max_candidates, use mask
  2. Candidate features change every step: the DRL learns which features
     correlate with good long-term outcomes
  3. The heuristic scores are FEATURES, not decisions -- the DRL can learn
     to weight them differently than a hand-crafted rule would

4.2 Curriculum Learning Strategy

Phase 1: Train on easy instances (few items, small bins)
  - Bin: 10x10x10, items: 5-20 per episode
  - Buffer: 3 items
  - This lets the agent learn basic packing behavior

Phase 2: Scale up bin size and item count
  - Bin: 20x20x20, items: 20-50
  - Buffer: 5 items

Phase 3: Full scale with all constraints
  - Bin: 40x40x40, items: 50-200
  - Buffer: 10 items
  - 2-bounded space
  - Full stability constraints

This curriculum prevents the agent from being overwhelmed early on.
"""

# def train_hybrid_agent():
#     \"\"\"
#     Training loop for the hybrid heuristic+DRL agent.
#
#     Key differences from Deep-Pack training (Section IV of paper):
#       1. Candidates generated per step (not fixed action space)
#       2. Multi-channel heightmap state
#       3. Buffer and multi-bin support
#       4. Curriculum learning over phases
#     \"\"\"
#     # Phase 1: Simple setting
#     config = {
#         'bin_size': (10, 10, 10),
#         'max_items': 20,
#         'buffer_size': 3,
#         'num_bins': 1,  # single bin first
#         'min_support': 0.3,  # lenient stability
#         'episodes': 100_000,
#     }
#
#     agent = HybridCandidateScoringNet(in_channels=3, grid_w=10, grid_d=10)
#     optimizer = torch.optim.Adam(agent.parameters(), lr=1e-3)
#     replay = ReplayMemory(capacity=50_000)
#
#     for episode in range(config['episodes']):
#         # Initialize environment
#         bin_state = create_empty_bin(*config['bin_size'])
#         buffer = ItemBuffer(config['buffer_size'])
#         items = generate_random_items(config['max_items'], *config['bin_size'])
#         buffer.load_conveyor(items)
#
#         while not buffer.is_empty():
#             # Select item from buffer (heuristic or learned)
#             selected_item = select_from_buffer(buffer, bin_state)
#
#             # Generate candidates
#             candidates = generate_candidates(bin_state, selected_item,
#                                               min_support=config['min_support'])
#
#             if not candidates:
#                 # No feasible placement -- discard or end
#                 buffer.discard_item(0)
#                 continue
#
#             # Construct state
#             state = construct_heightmap_state(bin_state, selected_item)
#
#             # Get candidate features and mask
#             cand_features, cand_mask = encode_candidates(candidates, max_k=100)
#
#             # Epsilon-greedy action selection
#             # ... (standard DQN logic)
#
#             # Execute action, compute reward, store transition
#             # ... (see deep_rl/deep_pack_3d_coding_ideas.py Section 6)
#
#             pass  # detailed implementation follows DQN training standard
#
#     # Phase 2 and 3: repeat with larger config
#     # ... (scale up bin_size, max_items, buffer_size, etc.)


# =============================================================================
# SECTION 5: HYPER-HEURISTIC INTERPRETATION
# =============================================================================

"""
5.1 This Hybrid System IS a Learned Selective Hyper-Heuristic

From overview KB Section 14:
  "Selective HH: Choose the best heuristic from a set of available heuristics"
  "No selective HH has been applied to 3D-PPs. This is a major opportunity."

Our system effectively implements a selective hyper-heuristic:
  - Available heuristics: DBLF, corner distance, DFTRC, support-maximizing,
    contact-maximizing, variance-minimizing
  - Each candidate is scored by ALL heuristics (as features)
  - The DRL agent LEARNS which heuristic to trust in each state
  - This is "learning to select heuristics" -- exactly a selective HH

Key advantage: the DRL can learn non-obvious combinations, e.g.:
  "When the bin is less than 50% full, prefer DBLF (back-bottom packing).
   When the bin is more than 50% full, prefer corner distance (fill gaps)."

This contributes to filling Research Gap 3 from the overview KB.

5.2 Thesis Contribution Angle

This hybrid approach can be framed as:
  "A learned selective hyper-heuristic for semi-online 3D bin packing
   with stability constraints"

This simultaneously addresses:
  - Gap 3: First selective HH for 3D-PPs
  - Gap 4: Bounded space online (2-bounded)
  - Gap 5: ML with practical constraints (stability)
  - Gap 6: Semi-online with buffer

This is a strong contribution profile for a thesis.
"""


# =============================================================================
# SECTION 6: ALTERNATIVE ARCHITECTURES TO CONSIDER
# =============================================================================

"""
6.1 Policy Gradient / Actor-Critic (instead of DQN)

Deep-Pack uses Double DQN (value-based). For the hybrid approach,
actor-critic methods may be more natural because:
  - Variable action spaces are handled more gracefully
  - PPO/A2C are more stable than DQN for complex environments
  - Zhao et al. (2020) successfully used actor-critic for 3D packing

Actor-critic architecture for our case:
  - Actor: outputs probability distribution over candidates
  - Critic: estimates V(s) or Q(s, a)
  - Both share the CNN backbone (visual feature extraction)
  - Constraint enforcement via feasibility masking on actor output

Recommended: PPO (Proximal Policy Optimization)
  - Stable training
  - Works well with discrete action spaces
  - Easy to implement with standard RL libraries (Stable-Baselines3, RLlib)


6.2 Attention-Based Architecture

Instead of scoring each candidate independently, use attention to
score candidates relative to each other:
  - Self-attention among candidates: each candidate "attends" to others
  - Cross-attention between visual state and candidates
  - Potentially better at learning spatial relationships between candidates

This is more complex but could capture "if I place here, the remaining
space works better for future items" reasoning.


6.3 Graph Neural Network for Candidate Evaluation

Represent the packing state as a graph:
  - Nodes: placed items + current candidates
  - Edges: adjacency relationships (touching items)
  - GNN processes the graph and scores each candidate node

This captures structural relationships that CNNs might miss.
However, CNNs on heightmaps are simpler and likely sufficient for
our bin sizes (up to 40x40).
"""


# =============================================================================
# SECTION 7: EXPERIMENTAL COMPARISON PLAN
# =============================================================================

"""
7.1 Baseline Methods to Compare Against

For a thorough thesis evaluation, compare the hybrid DRL approach against:

Pure heuristics (from overview KB Section 10):
  1. DBLF (Deepest-Bottom-Left with Fill)
  2. Best Fit (scan all bins, place in tightest fit)
  3. Shelf Fit (divide bin into shelves)
  4. Skyline method
  5. Corner Distance rule (Zhu & Lim 2012)
  6. DFTRC (Goncalves & Resende 2013)

ML baselines:
  7. Pure Deep-Pack (extended to 3D, pixel-level actions)
  8. DQN without heuristic filtering (Verma et al. style but without pre-filter)

Hybrid:
  9. Our method: heuristic-filtered DRL
  10. Random selection among feasible candidates (ablation)

7.2 Metrics

Primary:
  - Volume utilization: total_item_volume / total_bin_volume_used
  - Fill rate: same as volume utilization but per individual bin

Secondary:
  - Stability score: mean support ratio across all placements
  - CoG balance: mean center-of-gravity deviation from center
  - Items discarded: fraction of items that couldn't be placed
  - Bins used: total number of bins consumed (fewer = better)

Computational:
  - Inference time per step (ms)
  - Training time to convergence (hours)
  - Memory usage (MB)

7.3 Instance Generation

Random instances with configurable parameters:
  - Item size distribution: uniform, normal, bimodal, skewed
  - Item count: 50, 100, 200 items per episode
  - Size range: items from 1x1x1 to W/2 x D/2 x H/2
  - Weight: uniform or correlated with volume

This allows testing robustness across different item distributions,
which is important for a thesis-quality evaluation.
"""
